{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f59b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: torch in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: sqlglot in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (28.3.0)\n",
      "Collecting accelerate==0.25.0\n",
      "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from accelerate==0.25.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from accelerate==0.25.0) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from accelerate==0.25.0) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from accelerate==0.25.0) (6.0.3)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from accelerate==0.25.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from accelerate==0.25.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from huggingface-hub->accelerate==0.25.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from huggingface-hub->accelerate==0.25.0) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.25.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.25.0) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.25.0) (2025.11.12)\n",
      "Using cached accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.12.0\n",
      "    Uninstalling accelerate-1.12.0:\n",
      "      Successfully uninstalled accelerate-1.12.0\n",
      "Successfully installed accelerate-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "trl 0.26.1 requires accelerate>=1.4.0, but you have accelerate 0.25.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.34.1\n",
      "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: trl in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (0.26.1)\n",
      "Requirement already satisfied: peft==0.6.2 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.34.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (2.32.5)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.1)\n",
      "  Using cached tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from transformers==4.34.1) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from peft==0.6.2) (5.9.7)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from peft==0.6.2) (2.5.1+cu121)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from peft==0.6.2) (0.25.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (4.15.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.1)\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (0.70.15)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from datasets) (3.13.2)\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.6.2)\n",
      "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.25.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.23.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is still looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading trl-0.22.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.22.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.20.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading trl-0.19.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading trl-0.18.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.18.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting rich (from trl)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.15.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.15.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.11.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.11.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.11.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from trl) (1.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from aiohttp->datasets) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->transformers==4.34.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->transformers==4.34.1) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from requests->transformers==4.34.1) (2025.11.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.6.2) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers==4.34.1) (0.4.6)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from tyro>=0.5.11->trl) (0.17.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from tyro>=0.5.11->trl) (4.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.6.2) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moritz\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "Using cached tokenizers-0.14.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading trl-0.10.1-py3-none-any.whl (280 kB)\n",
      "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, huggingface-hub, tokenizers, transformers, datasets, trl\n",
      "\n",
      "  Attempting uninstall: fsspec\n",
      "\n",
      "    Found existing installation: fsspec 2025.9.0\n",
      "\n",
      "    Uninstalling fsspec-2025.9.0:\n",
      "\n",
      "      Successfully uninstalled fsspec-2025.9.0\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [fsspec]\n",
      "   ----- ---------------------------------- 1/7 [fsspec]\n",
      "   ----- ---------------------------------- 1/7 [fsspec]\n",
      "  Attempting uninstall: huggingface-hub\n",
      "   ----- ---------------------------------- 1/7 [fsspec]\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "   ----- ---------------------------------- 1/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "  Attempting uninstall: tokenizers\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "   ----------- ---------------------------- 2/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [tokenizers]\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "   ----------------- ---------------------- 3/7 [tokenizers]\n",
      "   ----------------- ---------------------- 3/7 [tokenizers]\n",
      "  Attempting uninstall: transformers\n",
      "   ----------------- ---------------------- 3/7 [tokenizers]\n",
      "    Found existing installation: transformers 4.57.3\n",
      "   ----------------- ---------------------- 3/7 [tokenizers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "    Uninstalling transformers-4.57.3:\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "      Successfully uninstalled transformers-4.57.3\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "  Attempting uninstall: datasets\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "    Found existing installation: datasets 4.4.1\n",
      "   ---------------------- ----------------- 4/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "    Uninstalling datasets-4.4.1:\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "      Successfully uninstalled datasets-4.4.1\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "  Attempting uninstall: trl\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "    Found existing installation: trl 0.26.1\n",
      "   ---------------------------- ----------- 5/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "    Uninstalling trl-0.26.1:\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "      Successfully uninstalled trl-0.26.1\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "   ---------------------------------- ----- 6/7 [trl]\n",
      "   ---------------------------------------- 7/7 [trl]\n",
      "\n",
      "Successfully installed datasets-2.14.7 fsspec-2023.10.0 huggingface-hub-0.17.3 pyarrow-hotfix-0.7 tokenizers-0.14.1 transformers-4.34.1 trl-0.10.1\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\moritz\\anaconda3\\envs\\granite_sql\\lib\\site-packages (1.26.4)\n",
      "Collecting pyarrow==14.0.0\n",
      "  Using cached pyarrow-14.0.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Using cached pyarrow-14.0.0-cp310-cp310-win_amd64.whl (24.6 MB)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 22.0.0\n",
      "    Uninstalling pyarrow-22.0.0:\n",
      "      Successfully uninstalled pyarrow-22.0.0\n",
      "Successfully installed pyarrow-14.0.0\n",
      "Installation Complete. Restarting runtime if in Colab/Jupyter is recommended.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install core dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch sqlglot accelerate==0.25.0\n",
    "\n",
    "# 2. Install HuggingFace libraries with compatible versions\n",
    "!pip install transformers==4.34.1 datasets trl peft==0.6.2\n",
    "\n",
    "# 3. Handle data library conflicts by downgrading numpy/pyarrow just in case (though recent pip should handle it)\n",
    "!pip install numpy==1.26.4 pyarrow==14.0.0\n",
    "\n",
    "# 4. Check installation (optional, but good practice)\n",
    "print(\"Installation Complete. Restarting runtime if in Colab/Jupyter is recommended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113c0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0 24.9M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 24.9M  100 24.9M    0     0  11.6M      0  0:00:02  0:00:02 --:--:-- 55.8M\n"
     ]
    }
   ],
   "source": [
    "! curl -L -o data.tar.bz2 https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2\n",
    "! tar -xjf data.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8e4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sqlglot\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from peft import LoraConfig\n",
    "import json \n",
    "import sqlglot\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "MODEL_NAME = \"ibm-granite/granite-4.0-350m-base\"\n",
    "OUTPUT_DIR = \"E:/training_runs/granite-grpo-wikisql\"\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "MAX_COMPLETION_LENGTH = 128\n",
    "LOCAL_DATA_DIR = \"data\"\n",
    "\n",
    "# Feature Schema \n",
    "WIKISQL_FEATURES = Features({\n",
    "    \"phase\": Value(\"int32\"),\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"sql\": Value(\"string\"),\n",
    "    \"table\": Value(\"string\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba09407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loading and preparation complete. Training subset size: 500\n",
      "First example:\n",
      "{'phase': 1, 'question': 'Tell me what the notes are for South Australia ', 'sql': '{\"sel\": 5, \"conds\": [[3, 0, \"SOUTH AUSTRALIA\"]], \"agg\": 0}', 'table': '{\"header\": [\"State/territory\", \"Text/background colour\", \"Format\", \"Current slogan\", \"Current series\", \"Notes\"], \"types\": [\"text\", \"text\", \"text\", \"text\", \"text\", \"text\"], \"rows\": [[\"Australian Capital Territory\", \"blue/white\", \"Yaa\\\\u00b7nna\", \"ACT \\\\u00b7 CELEBRATION OF A CENTURY 2013\", \"YIL\\\\u00b700A\", \"Slogan screenprinted on plate\"], [\"New South Wales\", \"black/yellow\", \"aa\\\\u00b7nn\\\\u00b7aa\", \"NEW SOUTH WALES\", \"BX\\\\u00b799\\\\u00b7HI\", \"No slogan on current series\"], [\"New South Wales\", \"black/white\", \"aaa\\\\u00b7nna\", \"NSW\", \"CPX\\\\u00b712A\", \"Optional white slimline series\"], [\"Northern Territory\", \"ochre/white\", \"Ca\\\\u00b7nn\\\\u00b7aa\", \"NT \\\\u00b7 OUTBACK AUSTRALIA\", \"CB\\\\u00b706\\\\u00b7ZZ\", \"New series began in June 2011\"], [\"Queensland\", \"maroon/white\", \"nnn\\\\u00b7aaa\", \"QUEENSLAND \\\\u00b7 SUNSHINE STATE\", \"999\\\\u00b7TLG\", \"Slogan embossed on plate\"], [\"South Australia\", \"black/white\", \"Snnn\\\\u00b7aaa\", \"SOUTH AUSTRALIA\", \"S000\\\\u00b7AZD\", \"No slogan on current series\"], [\"Victoria\", \"blue/white\", \"aaa\\\\u00b7nnn\", \"VICTORIA - THE PLACE TO BE\", \"ZZZ\\\\u00b7562\", \"Current series will be exhausted this year\"]], \"name\": \"table_1000181_1\", \"page_title\": NaN, \"section_title\": NaN, \"caption\": NaN, \"page_id\": NaN, \"id\": \"1-1000181-1\"}'}\n"
     ]
    }
   ],
   "source": [
    "# --- Load main data files (.jsonl) and table schemas (.tables.jsonl) ---\n",
    "\n",
    "\"\"\"\n",
    "Loading the local jsonl files\n",
    "dev -> Validation \n",
    "train -> train \n",
    "\n",
    "The dev and train json files have a table_id key so we know which table was referenced and the table json has all the headers page titles, rows and so on\n",
    "\"\"\"\n",
    "df_main_train = pd.read_json(f\"{LOCAL_DATA_DIR}/train.jsonl\", lines=True)\n",
    "df_main_dev = pd.read_json(f\"{LOCAL_DATA_DIR}/dev.jsonl\", lines=True)\n",
    "df_table_train = pd.read_json(f\"{LOCAL_DATA_DIR}/train.tables.jsonl\", lines=True)\n",
    "df_table_dev = pd.read_json(f\"{LOCAL_DATA_DIR}/dev.tables.jsonl\", lines=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create dictionary with ID as index and all of the other table info as key value pairs for this dict \n",
    "e.g \n",
    "\n",
    "{\n",
    "    # TABLE ID \n",
    "    \"1-1000181-1\": { \n",
    "        \n",
    "        # COLUMN NAMES ARE INNER VALUES\n",
    "        \"header\": [\"State/territory\", \"Text/background colour\", \"Format\", \"Current slogan\", \"Current series\", \"Notes\"],\n",
    "        \"types\": [\"text\", \"text\", \"text\", \"text\", \"text\", \"text\"],\n",
    "        \"rows\": [\n",
    "            [\"Australian Capital Territory\", \"blue/white\", \"Yaa\\u00b7nna\", ...],\n",
    "            ... # All other rows \n",
    "        ],\n",
    "        \"name\": \"table_1000181_1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "Why? We want to look up the table quickly when we \n",
    "\"\"\"\n",
    "table_dict_train = df_table_train.set_index('id').T.to_dict('dict') \n",
    "table_dict_dev = df_table_dev.set_index('id').T.to_dict('dict')\n",
    "\n",
    "def enforce_sql_types(sql_dict):\n",
    "    \"\"\"\n",
    "     The third value was sometimes int, sometimes string, ... \n",
    "    \"\"\"\n",
    "    if 'conds' in sql_dict:\n",
    "        new_conds = []\n",
    "        for cond in sql_dict['conds']:\n",
    "            if len(cond) == 3:\n",
    "                # Cast the third element to string\n",
    "                cond[2] = str(cond[2])\n",
    "            new_conds.append(cond)\n",
    "        sql_dict['conds'] = new_conds\n",
    "    return sql_dict\n",
    "\n",
    "def restructure_table(row):\n",
    "    \"\"\"\n",
    "     Make sure the merged table is well formed \n",
    "    \"\"\"\n",
    "    table_info = row['table']\n",
    "    # Handle NaN/missing values from map operation\n",
    "    if pd.isna(table_info) or isinstance(table_info, float): \n",
    "         table_info = {'header': [], 'types': [], 'rows': [], 'id': row['table_id']}\n",
    "    else:\n",
    "         # Ensure the ID is present in the table info dictionary\n",
    "         table_info['id'] = row['table_id'] \n",
    "    return table_info\n",
    "\n",
    "def merge_and_serialize(df_main, table_dict, is_train_subset=False):\n",
    "    if is_train_subset:\n",
    "        df_subset = df_main.head(500).copy()  # df_main.head(2000).copy() \n",
    "    else:\n",
    "        df_subset = df_main.copy()\n",
    "\n",
    "    # Merge the table data based on the table id\n",
    "    df_subset['table'] = df_subset['table_id'].map(table_dict)\n",
    "    df_subset['table'] = df_subset.apply(restructure_table, axis=1)\n",
    "\n",
    "    # make sure the sql part has strings in the conditions. \n",
    "    df_subset['sql'] = df_subset['sql'].apply(enforce_sql_types)\n",
    "\n",
    "    # make sure these columns are strings again not json. \n",
    "    df_subset['sql'] = df_subset['sql'].apply(json.dumps)\n",
    "    df_subset['table'] = df_subset['table'].apply(json.dumps)\n",
    "\n",
    "    # creates HF Dataset object. \n",
    "    return Dataset.from_pandas(df_subset.drop(columns=['table_id']), features=WIKISQL_FEATURES)\n",
    "\n",
    "train_dataset = merge_and_serialize(df_main_train, table_dict_train, is_train_subset=True)\n",
    "validation_dataset = merge_and_serialize(df_main_dev, table_dict_dev)\n",
    "\n",
    "# The final dataset is the subsetted train_dataset\n",
    "dataset = train_dataset\n",
    "\n",
    "print(f\"âœ… Data loading and preparation complete. Training subset size: {len(dataset)}\")\n",
    "print(\"First example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe8c4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 7812.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. Data Formatting and Reward Functions\n",
    "# ==========================================\n",
    "\n",
    "\"\"\"\n",
    "def format_wikisql(example):\n",
    "\"\"\"\n",
    "#Formats the input to include the table schema and deserializes the JSON strings.\n",
    "\"\"\"\n",
    "    # Deserialize the 'table' JSON string back into a dictionary\n",
    "    table_dict = json.loads(example[\"table\"])\n",
    "    \n",
    "    table_header = table_dict[\"header\"] \n",
    "    question = example[\"question\"]\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Generate a SQL query to answer the question based on the table schema.\\n\"\n",
    "        f\"Schema: {table_header}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Wrap your answer in <sql> tags. For example: <sql>SELECT * FROM table</sql>.\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    # The 'table' key is still a string in the output, but we extract the header here.\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"schema_cols\": table_header # Passing the header directly for simplicity\n",
    "    }\n",
    "\"\"\"\n",
    "def format_wikisql_with_gold_target(example):\n",
    "    \"\"\"\n",
    "    Formats the input prompt and secretly embeds the ground truth SQL\n",
    "    to bypass the GRPOTrainer's restrictive keyword arguments.\n",
    "    \"\"\"\n",
    "    table_dict = json.loads(example[\"table\"])\n",
    "    \n",
    "    table_header = table_dict[\"header\"] \n",
    "    question = example[\"question\"]\n",
    "    # ðŸŒŸ NEW: Get the raw SQL JSON string\n",
    "    gold_sql_json_string = example[\"sql\"]\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Generate a SQL query to answer the question based on the table schema.\\n\"\n",
    "        f\"Schema: {table_header}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Wrap your answer in <sql> tags. For example: <sql>SELECT * FROM table</sql>.\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # ðŸŒŸ NEW: Embed the ground truth into the prompt string using hidden tags\n",
    "    # The model will learn to ignore this, but the reward function can find it.\n",
    "    prompt_with_target = f\"[GOLD_SQL]{gold_sql_json_string}[/GOLD_SQL]\\n{prompt}\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt_with_target, # Use the augmented prompt for training\n",
    "        \"schema_cols\": table_header\n",
    "    }\n",
    "\n",
    "# Map the formatting function\n",
    "# dataset = dataset.map(format_wikisql)\n",
    "dataset = dataset.map(format_wikisql_with_gold_target)\n",
    "\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    \"\"\"Reward 1: Format Compliance (use of <sql> tags).\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        if \"<sql>\" in completion and \"</sql>\" in completion:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0) \n",
    "    return rewards\n",
    "\n",
    "def sql_syntax_reward_func(completions, **kwargs):\n",
    "    \"\"\"Reward 2: SQL Syntax Validity (using sqlglot).\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            if \"<sql>\" in completion and \"</sql>\" in completion:\n",
    "                query = completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "            else:\n",
    "                query = completion.strip()\n",
    "            \n",
    "            sqlglot.parse(query, read=\"sqlite\")\n",
    "            rewards.append(2.0)\n",
    "        except Exception:\n",
    "            rewards.append(-1.0)\n",
    "    return rewards\n",
    "\n",
    "def schema_consistency_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward 3: Schema Consistency (no hallucinated columns).\"\"\"\n",
    "    rewards = []\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        try:\n",
    "            schema_part = prompt.split(\"Schema: \")[1].split(\"\\nQuestion\")[0]\n",
    "            # Safely parse the column names from the prompt string\n",
    "            valid_cols = set(c.strip().strip(\"'\").strip('\"').lower() for c in schema_part.strip(\"[]\").split(\",\") if c)\n",
    "            \n",
    "            if \"<sql>\" in completion and \"</sql>\" in completion:\n",
    "                query = completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "            else:\n",
    "                query = completion.strip()\n",
    "\n",
    "            parsed = sqlglot.parse_one(query, read=\"sqlite\")\n",
    "            used_cols = set(col.name.lower() for col in parsed.find_all(sqlglot.exp.Column))\n",
    "            \n",
    "            if \"*\" in used_cols: \n",
    "                used_cols.remove(\"*\")\n",
    "                \n",
    "            if used_cols.issubset(valid_cols):\n",
    "                rewards.append(1.5)\n",
    "            else:\n",
    "                rewards.append(-0.5)\n",
    "                \n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "            \n",
    "    return rewards\n",
    "\n",
    "def logical_structure_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward 4: Scores based on logical structure.\n",
    "    This version extracts the ground truth from the prompt string.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        score = 0.0\n",
    "        \n",
    "        # --- 1. Extract Ground Truth from Prompt ---\n",
    "        try:\n",
    "            # Find the hidden GOLD_SQL tag in the prompt\n",
    "            start_idx = prompt.find(\"[GOLD_SQL]\") + len(\"[GOLD_SQL]\")\n",
    "            end_idx = prompt.find(\"[/GOLD_SQL]\")\n",
    "            gt_json_str = prompt[start_idx:end_idx].strip()\n",
    "            gt_dict = json.loads(gt_json_str)\n",
    "        except Exception:\n",
    "            # If we can't find or parse the gold target, we can't apply this reward.\n",
    "            rewards.append(0.0) \n",
    "            continue\n",
    "            \n",
    "        # 2. Extract Generated Query\n",
    "        try:\n",
    "            query_string = completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "            query_string_upper = query_string.upper()\n",
    "        except Exception:\n",
    "            rewards.append(-3.0) # Penalty for broken format\n",
    "            continue\n",
    "\n",
    "        # 3. Parse Ground Truth Requirements\n",
    "        gt_has_where = 'conds' in gt_dict and len(gt_dict['conds']) > 0\n",
    "        gt_has_agg = gt_dict.get('agg', 0) > 0 \n",
    "\n",
    "        # 4. Score Logic (The scoring logic remains the same)\n",
    "        if \"SELECT *\" in query_string_upper: score -= 1.0\n",
    "            \n",
    "        if gt_has_where:\n",
    "            if 'WHERE' in query_string_upper: score += 3.0 \n",
    "            else: score -= 4.0 \n",
    "            \n",
    "        agg_keywords = ['COUNT', 'AVG', 'SUM', 'MAX', 'MIN']\n",
    "        generated_has_agg = any(agg in query_string_upper for agg in agg_keywords)\n",
    "\n",
    "        if gt_has_agg:\n",
    "            if generated_has_agg: score += 3.0\n",
    "            else: score -= 4.0\n",
    "                \n",
    "        rewards.append(score)\n",
    "            \n",
    "    return rewards\n",
    "\n",
    "def intent_accuracy_reward_func(prompts, completions, **kwargs):\n",
    "    rewards = []\n",
    "    \n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        score = 0.0\n",
    "        \n",
    "        # --- 1. Extract Ground Truth & Schema from Prompt ---\n",
    "        try:\n",
    "            # Extract Gold JSON\n",
    "            gt_start = prompt.find(\"[GOLD_SQL]\") + len(\"[GOLD_SQL]\")\n",
    "            gt_end = prompt.find(\"[/GOLD_SQL]\")\n",
    "            gt_data = json.loads(prompt[gt_start:gt_end].strip())\n",
    "            \n",
    "            # Extract Schema to map indices to names\n",
    "            # format: \"Schema: ['State', 'Slogan', ...]\"\n",
    "            schema_start = prompt.find(\"Schema: \") + len(\"Schema: \")\n",
    "            schema_end = prompt.find(\"\\nQuestion\")\n",
    "            # a safe eval to turn string list \"['a', 'b']\" into list ['a', 'b']\n",
    "            schema_cols = eval(prompt[schema_start:schema_end].strip()) \n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # --- 2. Parse Generated Query ---\n",
    "        try:\n",
    "            # Extract SQL and normalize\n",
    "            gen_sql = completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "            gen_parsed = sqlglot.parse_one(gen_sql)\n",
    "            gen_sql_upper = gen_sql.upper()\n",
    "        except Exception:\n",
    "            rewards.append(-1.0) # Penalty for unparseable SQL\n",
    "            continue\n",
    "\n",
    "        # --- 3. Check Aggregation Accuracy (Fixes \"COUNT\" issues) ---\n",
    "        # Map WikiSQL agg indices to keywords: 0=None, 1=MAX, 2=MIN, 3=COUNT, 4=SUM, 5=AVG\n",
    "        agg_map = {0: None, 1: 'MAX', 2: 'MIN', 3: 'COUNT', 4: 'SUM', 5: 'AVG'}\n",
    "        target_agg = agg_map.get(gt_data.get('agg', 0))\n",
    "\n",
    "        if target_agg:\n",
    "            # If Gold has aggregation, Generation MUST have it\n",
    "            if target_agg in gen_sql_upper:\n",
    "                score += 1.0\n",
    "            else:\n",
    "                score -= 1.0 # Penalty for missing it\n",
    "        else:\n",
    "            # If Gold has NO aggregation, Generation should NOT have it (prevent hallucinations)\n",
    "            # We check if any agg keyword is present\n",
    "            if any(x in gen_sql_upper for x in ['MAX(', 'MIN(', 'COUNT(', 'SUM(', 'AVG(']):\n",
    "                score -= 0.5 \n",
    "\n",
    "        # --- 4. Check Column Selection Accuracy (Fixes \"Wrong selection\") ---\n",
    "        # The Gold JSON tells us the index of the SELECT column\n",
    "        sel_col_idx = gt_data.get('sel')\n",
    "        if sel_col_idx is not None and sel_col_idx < len(schema_cols):\n",
    "            target_col_name = schema_cols[sel_col_idx].lower()\n",
    "            \n",
    "            # Check if the generated SQL selects this column\n",
    "            # We look for the column name in the generated string (simplified check)\n",
    "            # A strict check would use sqlglot to find the SELECT expression\n",
    "            if target_col_name in gen_sql.lower():\n",
    "                score += 1.0\n",
    "            else:\n",
    "                score -= 1.0 # Significant penalty for selecting the wrong column\n",
    "\n",
    "        # --- 5. Check Condition Accuracy (Fixes \"WHERE\" clauses) ---\n",
    "        gt_conds = gt_data.get('conds', [])\n",
    "        if gt_conds:\n",
    "            # Check if we have a WHERE clause\n",
    "            if \"WHERE\" not in gen_sql_upper:\n",
    "                score -= 1.0\n",
    "            else:\n",
    "                # Check if we are filtering by the RIGHT column\n",
    "                for cond in gt_conds:\n",
    "                    col_idx = cond[0]\n",
    "                    if col_idx < len(schema_cols):\n",
    "                        cond_col_name = schema_cols[col_idx].lower()\n",
    "                        if cond_col_name in gen_sql.lower():\n",
    "                            score += 1.0\n",
    "                        else:\n",
    "                            score -= 0.5 # Wrong filter column\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b245ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phase': 1,\n",
       " 'question': 'Tell me what the notes are for South Australia ',\n",
       " 'sql': '{\"sel\": 5, \"conds\": [[3, 0, \"SOUTH AUSTRALIA\"]], \"agg\": 0}',\n",
       " 'table': '{\"header\": [\"State/territory\", \"Text/background colour\", \"Format\", \"Current slogan\", \"Current series\", \"Notes\"], \"types\": [\"text\", \"text\", \"text\", \"text\", \"text\", \"text\"], \"rows\": [[\"Australian Capital Territory\", \"blue/white\", \"Yaa\\\\u00b7nna\", \"ACT \\\\u00b7 CELEBRATION OF A CENTURY 2013\", \"YIL\\\\u00b700A\", \"Slogan screenprinted on plate\"], [\"New South Wales\", \"black/yellow\", \"aa\\\\u00b7nn\\\\u00b7aa\", \"NEW SOUTH WALES\", \"BX\\\\u00b799\\\\u00b7HI\", \"No slogan on current series\"], [\"New South Wales\", \"black/white\", \"aaa\\\\u00b7nna\", \"NSW\", \"CPX\\\\u00b712A\", \"Optional white slimline series\"], [\"Northern Territory\", \"ochre/white\", \"Ca\\\\u00b7nn\\\\u00b7aa\", \"NT \\\\u00b7 OUTBACK AUSTRALIA\", \"CB\\\\u00b706\\\\u00b7ZZ\", \"New series began in June 2011\"], [\"Queensland\", \"maroon/white\", \"nnn\\\\u00b7aaa\", \"QUEENSLAND \\\\u00b7 SUNSHINE STATE\", \"999\\\\u00b7TLG\", \"Slogan embossed on plate\"], [\"South Australia\", \"black/white\", \"Snnn\\\\u00b7aaa\", \"SOUTH AUSTRALIA\", \"S000\\\\u00b7AZD\", \"No slogan on current series\"], [\"Victoria\", \"blue/white\", \"aaa\\\\u00b7nnn\", \"VICTORIA - THE PLACE TO BE\", \"ZZZ\\\\u00b7562\", \"Current series will be exhausted this year\"]], \"name\": \"table_1000181_1\", \"page_title\": NaN, \"section_title\": NaN, \"caption\": NaN, \"page_id\": NaN, \"id\": \"1-1000181-1\"}',\n",
       " 'prompt': '[GOLD_SQL]{\"sel\": 5, \"conds\": [[3, 0, \"SOUTH AUSTRALIA\"]], \"agg\": 0}[/GOLD_SQL]\\nGenerate a SQL query to answer the question based on the table schema.\\nSchema: [\\'State/territory\\', \\'Text/background colour\\', \\'Format\\', \\'Current slogan\\', \\'Current series\\', \\'Notes\\']\\nQuestion: Tell me what the notes are for South Australia \\nWrap your answer in <sql> tags. For example: <sql>SELECT * FROM table</sql>.\\nAnswer:',\n",
       " 'schema_cols': ['State/territory',\n",
       "  'Text/background colour',\n",
       "  'Format',\n",
       "  'Current slogan',\n",
       "  'Current series',\n",
       "  'Notes']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf242c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO Training for Text-to-SQL with full logical rewards...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 52:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>-0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>-0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>-0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>-0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>-0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>-0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>-0.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'IF WHERE 2 THEN 1 ELSE 2 END AS name IN(SELECT DISTINCT columns FROM table WHERE column_of_query=2 O' contains unsupported syntax. Falling back to parsing as a 'Command'.\n",
      "'IF WHERE 2 THEN 1 ELSE 2 END AS name IN(SELECT DISTINCT columns FROM table WHERE column_of_query=2 O' contains unsupported syntax. Falling back to parsing as a 'Command'.\n",
      "'IF WHERE 2 THEN 1 ELSE 2 END AS name IN(SELECT DISTINCT columns FROM table WHERE column_of_query=2 O' contains unsupported syntax. Falling back to parsing as a 'Command'.\n",
      "'CALL thread_pthreads_thread_get_intermediate(false,(Index_Ldap)0) ON sajmiÅ¡te SERVED_BY<HOSTNAME>\n",
      "\n",
      "Y' contains unsupported syntax. Falling back to parsing as a 'Command'.\n",
      "'CALL thread_pthreads_thread_get_intermediate(false,(Index_Ldap)0) ON sajmiÅ¡te SERVED_BY<HOSTNAME>\n",
      "\n",
      "Y' contains unsupported syntax. Falling back to parsing as a 'Command'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved to E:/training_runs/granite-grpo-wikisql\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. Model & Trainer Initialization (CORRECTED)\n",
    "# ==========================================\n",
    "\n",
    "# Load Tokenizer (Keep this, as it's needed for the AutoModelForCausalLM and data prep)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "# GRPO Config\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[\n",
    "        format_reward_func, \n",
    "        sql_syntax_reward_func, \n",
    "        schema_consistency_reward_func,\n",
    "        logical_structure_reward_func,\n",
    "        intent_accuracy_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config\n",
    "    # REMOVED: tokenizer=tokenizer  <-- This was the unexpected keyword argument\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 7. Execution\n",
    "# ==========================================\n",
    "print(\"Starting GRPO Training for Text-to-SQL with full logical rewards...\")\n",
    "try:\n",
    "    trainer.train() \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user. Proceeding to final save...\")\n",
    "    \n",
    "# Save the final adapter\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(f\"Training complete. Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a50eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Tokenizer and Base Model...\n",
      "2. Loading and Merging LoRA Adapter...\n",
      "âœ… Model adapter merged and ready for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 77.55 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST CASE ---\n",
      "Question: How many schools did player number 3 play at?\n",
      "Ground Truth SQL: {'sel': 5, 'conds': [[1, 0, '3']], 'agg': 3}\n",
      "--- Generating Query ---\n",
      "\n",
      "--- RESULTS ---\n",
      "Generated Raw Text: <sql>SELECT COUNT(Player) FROM table WHERE Player = 3</sql>\n",
      "\n",
      "Player, No., Nationality, Position, Years in Toronto, School/Club Team\n",
      "3, England, England, Goalkeeper, University of St Andrews, St Andrews\n",
      "4, England, England, Goalkeeper, University of St Andrews, St Andrews\n",
      "5, England, England, Goalkeeper, University of St Andrews, St Andrews\n",
      "6, England, England, Goalkeeper, University of St Andrews, St Andrews\n",
      "7, England, England, Goalkeeper, University of St Andrews, St Andrews\n",
      "8, England, England, Goalkeeper\n",
      "Extracted SQL: SELECT COUNT(Player) FROM table WHERE Player = 3\n",
      "\n",
      "--- FINAL VERDICT ---\n",
      "SUCCESS: The model generated syntactically plausible SQL.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value\n",
    "import os # <-- IMPORTED OS FOR PATH JOINING\n",
    "\n",
    "# --- FIX 1: UPDATED OUTPUT_DIR to E: DRIVE ---\n",
    "# Assuming the adapter was saved here after the GRPO training run.\n",
    "OUTPUT_DIR = \"E:/training_runs/granite-grpo-wikisql\" \n",
    "MAX_PROMPT_LENGTH = 512\n",
    "MAX_COMPLETION_LENGTH = 128\n",
    "\n",
    "# --- FIX 2: Using os.path.join for robust local data access ---\n",
    "# This ensures the script finds the data files regardless of where the script is run from.\n",
    "LOCAL_DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "# --- Utility Functions (Must match the ones used in pre-processing) ---\n",
    "def format_wikisql(example):\n",
    "    \"\"\"Formats the input prompt for the model.\"\"\"\n",
    "    table_dict = json.loads(example[\"table\"])\n",
    "    table_header = table_dict[\"header\"] \n",
    "    question = example[\"question\"]\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Generate a SQL query to answer the question based on the table schema.\\n\"\n",
    "        f\"Schema: {table_header}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Wrap your answer in <sql> tags. For example: <sql>SELECT * FROM table</sql>.\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "def get_validation_data():\n",
    "    \"\"\"Simplified data loading for the dev set.\"\"\"\n",
    "    df_main_dev = pd.read_json(f\"{LOCAL_DATA_DIR}/dev.jsonl\", lines=True)\n",
    "    df_table_dev = pd.read_json(f\"{LOCAL_DATA_DIR}/dev.tables.jsonl\", lines=True)\n",
    "    table_dict_dev = df_table_dev.set_index('id').T.to_dict('dict')\n",
    "    \n",
    "    # We need the full merge and serialize here, but for brevity, we assume \n",
    "    # a function similar to merge_and_serialize was used to generate this.\n",
    "    \n",
    "    # For a quick test, we will just merge and serialize the first 10 dev samples:\n",
    "    df_subset = df_main_dev.head(10).copy()\n",
    "    df_subset['table'] = df_subset['table_id'].map(table_dict_dev)\n",
    "    # NOTE: In a full script, you would include restructure_table and enforce_sql_types here\n",
    "    \n",
    "    # Simple serialization for demonstration:\n",
    "    df_subset['sql'] = df_subset['sql'].apply(json.dumps)\n",
    "    df_subset['table'] = df_subset['table'].apply(json.dumps)\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df_subset.drop(columns=['table_id']), features=WIKISQL_FEATURES)\n",
    "    return dataset.map(format_wikisql)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Model Loading and Inference\n",
    "# ==========================================\n",
    "print(\"1. Loading Tokenizer and Base Model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"2. Loading and Merging LoRA Adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "print(\"âœ… Model adapter merged and ready for inference.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Testing Inference\n",
    "# ==========================================\n",
    "validation_dataset = get_validation_data()\n",
    "test_sample = validation_dataset[1] # Use the second example\n",
    "\n",
    "prompt = test_sample[\"prompt\"]\n",
    "ground_truth_sql = json.loads(test_sample[\"sql\"]) # Deserialize for comparison\n",
    "\n",
    "print(\"\\n--- TEST CASE ---\")\n",
    "print(f\"Question: {test_sample['question']}\")\n",
    "print(f\"Ground Truth SQL: {ground_truth_sql}\")\n",
    "print(\"--- Generating Query ---\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate the completion\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_COMPLETION_LENGTH,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode the output, stripping the input prompt\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_completion = generated_text[len(prompt):].strip()\n",
    "\n",
    "# --- Post-Processing ---\n",
    "try:\n",
    "    # Extract the SQL from between the tags (as done in the reward functions)\n",
    "    generated_sql = generated_completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "except IndexError:\n",
    "    generated_sql = \"Extraction Failed (Tags Missing)\"\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Generated Raw Text: {generated_completion}\")\n",
    "print(f\"Extracted SQL: {generated_sql}\")\n",
    "print(\"\\n--- FINAL VERDICT ---\")\n",
    "\n",
    "# Simple comparison for demonstration\n",
    "if generated_sql and generated_sql != \"Extraction Failed (Tags Missing)\":\n",
    "    print(\"SUCCESS: The model generated syntactically plausible SQL.\")\n",
    "else:\n",
    "    print(\"FAILURE: The model failed to adhere to the required format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15960d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST CASE 2 ---\n",
      "Question: What position does the player who played for butler cc (ks) play?\n",
      "Ground Truth SQL: {'sel': 3, 'conds': [[5, 0, 'Butler CC (KS)']], 'agg': 0}\n",
      "--- Generating Query ---\n",
      "\n",
      "--- RESULTS ---\n",
      "Extracted SQL: SELECT * FROM player WHERE nationality = 'ks' AND position = 'butler cc (ks)' AND years_in_toronto = 0 AND school/club_team = 'butler cc (ks)'\n"
     ]
    }
   ],
   "source": [
    "test_sample_1 = validation_dataset[0]\n",
    "\n",
    "prompt = test_sample_1[\"prompt\"]\n",
    "ground_truth_sql = json.loads(test_sample_1[\"sql\"])\n",
    "\n",
    "print(\"\\n--- TEST CASE 2 ---\")\n",
    "print(f\"Question: {test_sample_1['question']}\")\n",
    "print(f\"Ground Truth SQL: {ground_truth_sql}\")\n",
    "print(\"--- Generating Query ---\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate the completion\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_COMPLETION_LENGTH,\n",
    "        do_sample=False, # Use greedy decoding for predictable results\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_completion = generated_text[len(prompt):].strip()\n",
    "\n",
    "# --- Post-Processing ---\n",
    "try:\n",
    "    generated_sql = generated_completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "except IndexError:\n",
    "    generated_sql = \"Extraction Failed (Tags Missing)\"\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Extracted SQL: {generated_sql}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c3ffdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST CASE 3 ---\n",
      "Question: Who are all of the players on the Westchester High School club team?\n",
      "Ground Truth SQL: {'sel': 0, 'conds': [[5, 0, 'Westchester High School']], 'agg': 0}\n",
      "--- Generating Query ---\n",
      "\n",
      "--- RESULTS ---\n",
      "Extracted SQL: SELECT * FROM players WHERE nationality = 'Westchester High School' AND position = 'Player' AND years_in_toronto = 0 AND school/club_team = 'Toronto' AND nationality = 'Westchester High School' AND position = 'Player' AND years_in_toronto = 0\n"
     ]
    }
   ],
   "source": [
    "test_sample_5 = validation_dataset[5]\n",
    "\n",
    "prompt = test_sample_5[\"prompt\"]\n",
    "ground_truth_sql = json.loads(test_sample_5[\"sql\"])\n",
    "\n",
    "print(\"\\n--- TEST CASE 3 ---\")\n",
    "print(f\"Question: {test_sample_5['question']}\")\n",
    "print(f\"Ground Truth SQL: {ground_truth_sql}\")\n",
    "print(\"--- Generating Query ---\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate the completion\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_COMPLETION_LENGTH,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_completion = generated_text[len(prompt):].strip()\n",
    "\n",
    "# --- Post-Processing ---\n",
    "try:\n",
    "    generated_sql = generated_completion.split(\"<sql>\")[1].split(\"</sql>\")[0].strip()\n",
    "except IndexError:\n",
    "    generated_sql = \"Extraction Failed (Tags Missing)\"\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Extracted SQL: {generated_sql}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite_sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
